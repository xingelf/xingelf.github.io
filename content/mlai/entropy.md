---
title: 情報理論
date: "2021-08-01"
draft: false
categories:
  - "ML/AI"
tags:
  - "Math"
  - "E資格"
thumbnail: "/images/artificial-intelligence.png"

---

目的：情報理論の基本を理解する。


# 情報量

事象Aが発生する確率をP(A)とした時、情報量I(A)は以下で定義される。

$ I(A) = -\log P(A)$

確率が小さいほど情報量は大きい。
レアな事象ほどインパクト（情報）が大きいとイメージすると覚えやすい。

確率P(A)に対しては以下の特徴を持つ。

- 確率が小さい時、大きくなる
- 複数の独立な事象において、情報量はその和で表現される（確率は積で表現される）

$ I(A \cap B) = I(A) + I(B)$

# エントロピー

情報量の期待値で定義される。

$ H(A) = -P(A)\log P(A)$

# 交差エントロピー

確率分布$p(x)$と$q(x)$の交差エントロピーは以下で定義される。

$ H(p,q) = -\sum_{x} p(x)\log q(x)$


# KLダイバージェンス - Kullback-Leibler divergence

確率分布$p(x)$と$q(x)$のKLダイバージェンスは以下で定義される。

$ D_{KL}(p||q) = \sum_{x} p(x)\log \frac{p(x)}{q(x)}$

確率分布$p(x)$と$q(x)$に対して、非対称であるが、
2つの確率分布の類似性や距離みたいなイメージに近い。
$p(x)=q(x)$の場合は$\log$部分が1となるため、
$D(p||q)$はゼロになる。


## エントロピーと交差エントロピーとKLダイバージェンスの関係

この3つの量には以下の関係があります。

$ H(p,q) = H(p) + D_{KL}(p||q)$


---

# 参考動画 情報理論

{{< youtube ErfnhcEV1O8 >}}
